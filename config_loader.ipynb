{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e354fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload package\n",
    "import importlib\n",
    "import src.utils.config_loader\n",
    "importlib.reload(src.utils.config_loader)\n",
    "\n",
    "from src.utils.config_loader import ConfigLoader\n",
    "\n",
    "config_loader = ConfigLoader(\"configs\")\n",
    "base_configs = config_loader.load_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "76ea2670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "from typing import NamedTuple\n",
    "\n",
    "# ignore FutureWarning: DataFrameGroupBy.apply\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "class ProcessedData(NamedTuple):\n",
    "    comments: pd.DataFrame\n",
    "    replies: pd.DataFrame\n",
    "    user_pairs: pd.DataFrame\n",
    "    submissions: pd.DataFrame\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, base_configs: dict):\n",
    "        self.base_configs = base_configs\n",
    "\n",
    "        # Paths\n",
    "        self.raw_path = self.base_configs['paths']['raw']\n",
    "        self.processed_path = self.base_configs['paths']['processed']\n",
    "\n",
    "        # Maps\n",
    "        self.labels_map = self.base_configs['labels']\n",
    "        self.subreddits_map = self.base_configs['subreddits']\n",
    "        self.required_columns = self.base_configs['required_columns']\n",
    "        \n",
    "        # Processing configs\n",
    "        self.cleaning_cfgs = self.base_configs['cleaning']\n",
    "        self.temporal_cfgs = self.base_configs['temporal']\n",
    "        self.default_values = self.base_configs['default_values']\n",
    "        self.embedding_cfgs = self.base_configs['text_embedding']\n",
    "\n",
    "    def clean_data(self, raw_data: pd.DataFrame) -> pd.DataFrame:\n",
    "        processed = raw_data.copy()\n",
    "        # 1. Check if mandatory columns exist\n",
    "        missing_cols = set(self.required_columns) - set(processed.columns)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing mandatory columns: {missing_cols}\")\n",
    "\n",
    "        #=======================================\n",
    "        # CLEANING\n",
    "        #=======================================\n",
    "        # 2. Normalize subreddit names\n",
    "        if self.cleaning_cfgs.get('normalize_subreddits', False):\n",
    "            processed['subreddit'] = processed['subreddit'].str.lower()\n",
    "\n",
    "        # 3. Rename columns\n",
    "        if 'rename_columns' in self.cleaning_cfgs:\n",
    "            processed = processed.rename(columns=self.cleaning_cfgs['rename_columns'])\n",
    "\n",
    "        # 4. Map label and subreddit to IDs\n",
    "        processed['label_desc'] = processed['label'].map(self.labels_map)\n",
    "        processed['subreddit_id'] = processed['subreddit'].map(self.subreddits_map)\n",
    "\n",
    "        # 5. Parse timestamps\n",
    "        if 'timestamp_parsing' in self.cleaning_cfgs:\n",
    "            timestamp_cfg = self.cleaning_cfgs['timestamp_parsing']\n",
    "            \n",
    "            # Extract valid pandas parameters\n",
    "            pd_params = {}\n",
    "            if 'dayfirst' in timestamp_cfg:\n",
    "                pd_params['dayfirst'] = timestamp_cfg['dayfirst']\n",
    "            if 'primary_format' in timestamp_cfg:\n",
    "                pd_params['format'] = timestamp_cfg['primary_format']\n",
    "            if 'error_handling' in timestamp_cfg and timestamp_cfg['error_handling'] == 'coerce':\n",
    "                pd_params['errors'] = 'coerce'\n",
    "            \n",
    "            try:\n",
    "                processed['timestamp'] = pd.to_datetime(processed['timestamp'], **pd_params)\n",
    "            except:\n",
    "                # If primary format fails, try without format specification\n",
    "                pd_params.pop('format', None)\n",
    "                processed['timestamp'] = pd.to_datetime(processed['timestamp'], **pd_params)\n",
    "            # CHECK TYPE\n",
    "            if not pd.api.types.is_datetime64_any_dtype(processed['timestamp']):\n",
    "                raise ValueError(\"Timestamp conversion failed\")\n",
    "            \n",
    "        # 6. Remove self-replies\n",
    "        if self.cleaning_cfgs.get('remove_self_replies', False):\n",
    "            processed = processed[processed['src_author'] != processed['dst_author']]\n",
    "\n",
    "        # 7. Filter final columns\n",
    "        final_cols = [\n",
    "            'subreddit_id', 'subreddit', 'timestamp',\n",
    "            'submission_id', 'submission_text', 'label', 'label_desc',\n",
    "            'src_author', 'src_comment_id', 'src_comment_text',\n",
    "            'dst_author', 'dst_comment_id', 'dst_comment_text',\n",
    "            'agreement_fraction', 'individual_kappa'\n",
    "        ]\n",
    "        return processed[final_cols]\n",
    "    \n",
    "    def process_comments(self, replies: pd.DataFrame | None = None) -> pd.DataFrame:\n",
    "        #=======================================\n",
    "        # HELPER FUNCTIONS\n",
    "        #=======================================\n",
    "        def _extract_unique_comments(replies):\n",
    "            comments_list = []\n",
    "            # Source comments (child)\n",
    "            src_comments = replies[['subreddit_id', 'subreddit', 'timestamp', 'submission_id', 'submission_text',\n",
    "                              'src_author', 'src_comment_id', 'src_comment_text']].copy()\n",
    "            src_comments = src_comments.rename(columns={\n",
    "                'src_author': 'author',\n",
    "                'src_comment_id': 'comment_id', \n",
    "                'src_comment_text': 'comment_text'\n",
    "            })\n",
    "            src_comments['is_parent'] = False\n",
    "            \n",
    "            # Destination comments (parents)\n",
    "            dst_comments = replies[['subreddit_id', 'subreddit', 'timestamp', 'submission_id', 'submission_text',\n",
    "                              'dst_author', 'dst_comment_id', 'dst_comment_text']].copy()\n",
    "            dst_comments = dst_comments.rename(columns={\n",
    "                'dst_author': 'author',\n",
    "                'dst_comment_id': 'comment_id', \n",
    "                'dst_comment_text': 'comment_text'\n",
    "            })\n",
    "            dst_comments['is_parent'] = True\n",
    "\n",
    "            # Combine and remove duplicates\n",
    "            all_comments = pd.concat([src_comments, dst_comments])\n",
    "            unique_comments = all_comments.drop_duplicates(subset=['comment_id'])\n",
    "            return unique_comments\n",
    "        \n",
    "        def _apply_fixed_windows(days: int, df: pd.DataFrame):\n",
    "            df = df.copy().sort_values(by='timestamp')\n",
    "            min_date = df['timestamp'].min()\n",
    "            max_date = df['timestamp'].max()\n",
    "            window_edges = pd.date_range(start=min_date, end=max_date + pd.Timedelta(days=days), freq=f'{days}D')\n",
    "        \n",
    "            # Add timestep column\n",
    "            df['timestep'] = pd.cut(\n",
    "                df['timestamp'],\n",
    "                bins=window_edges,\n",
    "                labels=range(len(window_edges)-1),\n",
    "                right=False\n",
    "            )\n",
    "        \n",
    "            # Create interval labels\n",
    "            interval_labels = []\n",
    "            for i in range(len(window_edges)-1):\n",
    "                start = window_edges[i].date()\n",
    "                # For last interval, use actual max timestamp\n",
    "                if i == len(window_edges)-2:\n",
    "                    end = max_date.date()\n",
    "                else:\n",
    "                    end = (window_edges[i+1] - pd.Timedelta(days=1)).date()\n",
    "                interval_labels.append(f\"{start} - {end}\")\n",
    "        \n",
    "            df['interval'] = df['timestep'].apply(lambda x: interval_labels[int(x)] if pd.notnull(x) else None)\n",
    "            df['actual_window_size'] = df.groupby(['subreddit_id', 'interval'])['timestamp'].transform(\n",
    "                lambda ts: (ts.max() - ts.min()).days\n",
    "            )\n",
    "        \n",
    "            return df\n",
    "\n",
    "        def _apply_custom_subreddit_windows(subreddit_name, df, windows_dict):\n",
    "            days = windows_dict.get(subreddit_name, 178)\n",
    "            return _apply_fixed_windows(days, df)\n",
    "\n",
    "        def _merge_small_windows(df, min_window_size):\n",
    "            df = df.copy()\n",
    "            while True:\n",
    "                window_counts = df['interval'].value_counts().sort_index()\n",
    "                small_windows = window_counts[window_counts < min_window_size].index.to_list()\n",
    "                if not small_windows:\n",
    "                    break\n",
    "                all_intervals = sorted(df['interval'].dropna().unique())\n",
    "                merged_any = False\n",
    "                for sw in small_windows:\n",
    "                    if sw not in df['interval'].values:\n",
    "                        continue\n",
    "                    window_idx = all_intervals.index(sw)\n",
    "                    # Determine merge direction\n",
    "                    if window_idx == 0:\n",
    "                        target = all_intervals[1] if len(all_intervals) > 1 else None\n",
    "                    elif window_idx == len(all_intervals) - 1:\n",
    "                        target = all_intervals[-2] if len(all_intervals) > 1 else None\n",
    "                    else:\n",
    "                        prev_win = all_intervals[window_idx - 1]\n",
    "                        next_win = all_intervals[window_idx + 1]\n",
    "                        prev_count = len(df[df['interval'] == prev_win])\n",
    "                        next_count = len(df[df['interval'] == next_win])\n",
    "                        target = prev_win if prev_count <= next_count else next_win\n",
    "                    if target:\n",
    "                        target_rows = df[df['interval'] == target]\n",
    "                        if len(target_rows) == 0:\n",
    "                            print(f\"    + [subreddit: {df['subreddit_id'].iloc[0]}] Skipping merge: target interval '{target}' is empty.\")\n",
    "                            continue\n",
    "                        # Get all timestamps to update interval label\n",
    "                        merged_mask = (df['interval'].astype(str) == str(sw)) | (df['interval'].astype(str) == str(target))\n",
    "                        merged_timestamps = df.loc[merged_mask, 'timestamp']\n",
    "                        new_start = merged_timestamps.min().date()\n",
    "                        new_end = merged_timestamps.max().date()\n",
    "                        new_interval_label = f\"{new_start} - {new_end}\"\n",
    "                        target_timestep = target_rows['timestep'].iloc[0]\n",
    "                        \n",
    "                        # Print details\n",
    "                        # n_from_sw = (df['interval'] == sw).sum()\n",
    "                        # n_from_target = (df['interval'] == target).sum()\n",
    "                        # old_sw_timestep = df[df['interval'] == sw]['timestep'].unique()\n",
    "                        # old_target_timestep = df[df['interval'] == target]['timestep'].unique()\n",
    "                        # print(f\"Subreddit: {df['subreddit_id'].iloc[0]}\")\n",
    "                        # print(f\"    + Merging {n_from_sw} comments from '{sw}' (timestep:{old_sw_timestep[0]}) \"\n",
    "                        #     f\"and {n_from_target} from '{target}' (timestep:{old_target_timestep[0]}) \"\n",
    "                        #     f\"onto new interval '{new_interval_label}'\"\n",
    "                        # )\n",
    "                        \n",
    "                        # Update both intervals and timestep\n",
    "                        df.loc[merged_mask, 'interval'] = new_interval_label\n",
    "                        df.loc[merged_mask, 'timestep'] = target_timestep\n",
    "                        merged_any = True\n",
    "                if not merged_any:\n",
    "                    break\n",
    "            \n",
    "            # Recalculate intervals\n",
    "            intervals_sorted = sorted(df['interval'].dropna().unique())\n",
    "            interval_map = {interval: i for i, interval in enumerate(intervals_sorted)}\n",
    "            df['timestep'] = df['interval'].map(interval_map)\n",
    "            \n",
    "            # Recalculate actual_window_size after merging\n",
    "            df['actual_window_size'] = df.groupby('interval')['timestamp'].transform(\n",
    "                lambda ts: (ts.max() - ts.min()).days\n",
    "            )\n",
    "\n",
    "            return df\n",
    "        \n",
    "        #=======================================\n",
    "        # END HELPER FUNCTIONS\n",
    "        #=======================================\n",
    "        \n",
    "        comments = replies.copy()\n",
    "        # 1. Get unique comments from replies data\n",
    "        comments = _extract_unique_comments(replies)\n",
    "\n",
    "        #=======================================\n",
    "        # PROCESS COMMENTS\n",
    "        #=======================================\n",
    "        # 2. Infer parent comment timestamp\n",
    "        parent_time_cfg = self.temporal_cfgs.get('parent_time_inference', {})\n",
    "        if parent_time_cfg.get('infer_parent_comment_time', True):\n",
    "            delta_min = parent_time_cfg.get('delta_minutes', 30)\n",
    "            if 'is_parent' in comments.columns:\n",
    "                comments.loc[comments['is_parent'], 'timestamp'] -= pd.Timedelta(minutes=delta_min)\n",
    "        \n",
    "        # 3. Apply windowing\n",
    "        window_cfg = self.temporal_cfgs.get('windowing', {})\n",
    "        if window_cfg.get('enabled', True):\n",
    "            window_strategy = window_cfg.get('strategy', None)\n",
    "            strategy_configs = window_cfg.get(f'{window_strategy}_windows', {})\n",
    "        \n",
    "            if window_strategy == 'fixed':\n",
    "                days = strategy_configs.get('size', 178)\n",
    "                print(f\"Using fixed windows of {days} days\")\n",
    "                comments = comments.groupby('subreddit_id', group_keys=False).apply(\n",
    "                    lambda df: _apply_fixed_windows(days, df),\n",
    "                )\n",
    "            elif window_strategy == 'custom_subreddit':\n",
    "                print(f\"Using custom subreddit windows: {strategy_configs}\")\n",
    "                comments = comments.groupby('subreddit', group_keys=False).apply(\n",
    "                    lambda df: _apply_custom_subreddit_windows(df.subreddit.iloc[0], df, strategy_configs)\n",
    "                )\n",
    "\n",
    "            # 4. Handle merging of small windows\n",
    "            merge_cfg = window_cfg.get('merge_small_windows', {})\n",
    "            if merge_cfg.get('enabled', True):\n",
    "                min_window_size = merge_cfg.get('min_comments_per_window', 50)\n",
    "                print(f\"    + Merging small windows with minimum size: {min_window_size}\")\n",
    "                comments = comments.groupby('subreddit_id', group_keys=False).apply(\n",
    "                    lambda df: _merge_small_windows(df, min_window_size)\n",
    "                )\n",
    "\n",
    "        return comments\n",
    "\n",
    "    def process_replies(self, replies: pd.DataFrame, comments: pd.DataFrame) -> pd.DataFrame:\n",
    "        replies = replies.copy()\n",
    "        reply_comments = comments[comments['is_parent'] == False][['subreddit_id', 'comment_id', 'timestep', 'interval', 'actual_window_size']]\n",
    "        assert len(replies) == len(reply_comments), \"Replies and comments must have the same length\"\n",
    "\n",
    "        # 1. Left join replies on subreddit_id and comment_id\n",
    "        replies_temporal = replies.merge(\n",
    "            reply_comments,\n",
    "            left_on=['subreddit_id', 'src_comment_id'],\n",
    "            right_on=['subreddit_id', 'comment_id'],\n",
    "            how='left',\n",
    "        ).drop('comment_id', axis=1)\n",
    "        \n",
    "        # 2. Calculate confidence (agreement_fraction * individual_kappa)\n",
    "        default_confidence = self.default_values['confidence']\n",
    "        replies_temporal['confidence'] = replies_temporal['agreement_fraction'] * replies_temporal['individual_kappa'].fillna(default_confidence)\n",
    "        print(f\"    + Calculated confidence for {len(replies_temporal)} replies, number of NaNs: {replies_temporal['individual_kappa'].isna().sum()} (used default_confidence = {default_confidence})\")\n",
    "\n",
    "        # 3. Keep minimal final columns (drop _text columns)\n",
    "        replies_temporal = replies_temporal.loc[:, ~replies_temporal.columns.str.endswith('_text')]\n",
    "\n",
    "        return replies_temporal\n",
    "\n",
    "    def process_submissions(self, comments: pd.DataFrame | None = None):\n",
    "        comments = comments.copy()\n",
    "        # 1. Get unique submissions from comment data\n",
    "        submissions = comments.groupby(['subreddit_id', 'subreddit', 'submission_id']).agg({\n",
    "            'submission_text': 'first',\n",
    "            'timestamp': 'min',          \n",
    "            'timestep': 'first',         \n",
    "            'interval': 'first',         \n",
    "            'actual_window_size': 'first' \n",
    "        }).reset_index()\n",
    "        submissions = submissions.rename(columns={\n",
    "            'timestamp': 'first_comment_time',\n",
    "            'timestep': 'first_comment_timestep',\n",
    "            'interval': 'first_comment_interval',\n",
    "            'actual_window_size': 'first_comment_actual_window_size'\n",
    "        })\n",
    "        submissions = submissions[['subreddit_id', 'subreddit', 'submission_id', 'submission_text', \n",
    "                                'first_comment_time', 'first_comment_timestep', \n",
    "                                'first_comment_interval', 'first_comment_actual_window_size']]\n",
    "        print(f\"Total unique submissions: {len(submissions)}\")\n",
    "\n",
    "        return submissions\n",
    "    \n",
    "    def process_user_pairs(self, replies: pd.DataFrame) -> pd.DataFrame:\n",
    "        def _calculate_net_vector(labels, confidences):\n",
    "            vector = [0.0, 0.0, 0.0]  # [disagree, neutral, agree]\n",
    "            total_weight = 0.0            \n",
    "            for label, conf in zip(labels, confidences):\n",
    "                if label in [0, 1, 2]:\n",
    "                    vector[label] += conf\n",
    "                    total_weight += conf\n",
    "            # Normalize to sum to 1 (if any interactions exist)\n",
    "            if total_weight > 0:\n",
    "                vector = [round(x / total_weight, 6) for x in vector]\n",
    "            return vector\n",
    "\n",
    "        if replies.empty:\n",
    "            print(\"No replies data available to build user pairs.\")\n",
    "            return pd.DataFrame()\n",
    "        group_cols = ['subreddit_id', 'subreddit', 'timestep', 'interval', 'actual_window_size', 'src_author', 'dst_author']\n",
    "        agg_dict = {\n",
    "            'label': list,\n",
    "            'agreement_fraction': 'mean',\n",
    "            'individual_kappa': 'mean', \n",
    "            'confidence': list,  # Keep all confidence values per group\n",
    "        }\n",
    "\n",
    "        # Aggregate user pairs and round values to 3 decimals\n",
    "        pairs = replies.groupby(group_cols).agg(agg_dict).reset_index().rename(columns={\n",
    "            'agreement_fraction': 'mean_agreement_fraction',\n",
    "            'individual_kappa': 'mean_kappa',\n",
    "        })\n",
    "        pairs['mean_kappa'] = pairs['mean_kappa'].round(3)\n",
    "        pairs['mean_agreement_fraction'] = pairs['mean_agreement_fraction'].round(3)\n",
    "\n",
    "        # Calculate mean confidence and round to 3 decimals\n",
    "        pairs['mean_confidence'] = pairs['confidence'].apply(lambda x: round(np.mean(x), 3) if x else 0.0)\n",
    "\n",
    "        # Calculate net vectors using actual confidence values per interaction\n",
    "        net_vectors = []\n",
    "        for _, row in pairs.iterrows():\n",
    "            labels = row['label']\n",
    "            confidences = row['confidence']\n",
    "            net_vector = _calculate_net_vector(labels, confidences)\n",
    "            net_vectors.append(net_vector)\n",
    "        pairs['net_vector'] = net_vectors\n",
    "\n",
    "        # Remove intermedia columns\n",
    "        pairs = pairs.drop(columns=['label', 'confidence'])\n",
    "        print(f\"Built user pairs with {len(pairs)} interactions\")\n",
    "        print(f\"   + Unique src_authors: {pairs['src_author'].nunique()}, dst_authors: {pairs['dst_author'].nunique()}\")\n",
    "        print(f\"   + Rows with NaN kappa: {pairs['mean_kappa'].isna().sum()}\")\n",
    "        return pairs\n",
    "\n",
    "    def embed_text_column(self, device, df: pd.DataFrame, text_column: str, configs: dict) -> pd.DataFrame:\n",
    "        if not configs:\n",
    "            print(\"    + No embedding configurations provided, skipping embedding.\")\n",
    "            return df\n",
    "        texts = df[text_column].tolist()\n",
    "        if configs.get('type', 'sentence-transformers') == 'sentence-transformers':\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            model_name = configs.get('name', 'all-MiniLM-L6-v2')\n",
    "            batch_size = configs.get('batch_size', 128)\n",
    "            max_length = configs['max_length']\n",
    "\n",
    "            model = SentenceTransformer(model_name)\n",
    "            if max_length is not None:\n",
    "                model.max_seq_length = max_length\n",
    "            print(f\"    + Using embedding model: {model_name} max_length: {model.max_seq_length} batch size: {batch_size} on device: {model.device}\")\n",
    "\n",
    "            embeddings = model.encode(\n",
    "                texts, \n",
    "                batch_size=batch_size, \n",
    "                show_progress_bar=True, \n",
    "                convert_to_numpy=True,\n",
    "                device=device\n",
    "            )\n",
    "            print(f\"    + Embedding shape: {embeddings[0].shape}\")\n",
    "\n",
    "            # Add embeddings to df\n",
    "            df = df.copy()\n",
    "            df['embeddings'] = list(embeddings)\n",
    "\n",
    "            # Dump torch cache if used cuda\n",
    "            if device and str(device) != 'cpu':\n",
    "                import torch\n",
    "                torch.cuda.empty_cache()\n",
    "                print(\"    + Cleared CUDA cache after embedding.\")       \n",
    "        return df\n",
    "\n",
    "    def save_processed_data(self, file_type, data, file_path):\n",
    "        if file_type == 'csv':\n",
    "            pd.DataFrame.to_csv(data, file_path, index=False)\n",
    "        elif file_type == 'pickle':\n",
    "            pd.to_pickle(data, file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {file_type}. Use 'csv' or 'pickle'.\")\n",
    "    \n",
    "    # Print summary functions\n",
    "    def print_basic_reply_statistics(self, df, group_col='subreddit'):\n",
    "        stats = []\n",
    "        for sid, group in df.groupby(group_col):\n",
    "            earliest = group['timestamp'].min().date()\n",
    "            latest = group['timestamp'].max().date()\n",
    "            n_replies = len(group)\n",
    "            n_self = (group['src_author'] == group['dst_author']).sum()\n",
    "            pct_self = round(n_self / n_replies, 2) if n_replies else 0.0\n",
    "            pct_disagree = round((group['label_desc'] == 'disagree').mean() * 100, 1)\n",
    "            pct_neutral = round((group['label_desc'] == 'neutral').mean() * 100, 1)\n",
    "            pct_agree = round((group['label_desc'] == 'agree').mean() * 100, 1)\n",
    "            n_unique_comments = group['src_comment_id'].nunique() + group['dst_comment_id'].nunique()\n",
    "            n_unique_authors = group['src_author'].nunique() + group['dst_author'].nunique()\n",
    "            stats.append({\n",
    "                group_col: sid,\n",
    "                'Earliest Date': earliest,\n",
    "                'Latest Date': latest,\n",
    "                '# Replies': n_replies,\n",
    "                '% Self-Replies': pct_self,\n",
    "                '% Disagree': pct_disagree,\n",
    "                '% Neutral': pct_neutral,\n",
    "                '% Agree': pct_agree,\n",
    "                '# Unique Comments': n_unique_comments,\n",
    "                '# Unique Authors': n_unique_authors\n",
    "            })\n",
    "        # Add \"All\" row (use label_desc for consistency)\n",
    "        all_group = df\n",
    "        earliest = all_group['timestamp'].min().date()\n",
    "        latest = all_group['timestamp'].max().date()\n",
    "        n_replies = len(all_group)\n",
    "        n_self = (all_group['src_author'] == all_group['dst_author']).sum()\n",
    "        pct_self = round(n_self / n_replies, 2) if n_replies else 0.0\n",
    "        pct_disagree = round((all_group['label_desc'] == 'disagree').mean() * 100, 1)\n",
    "        pct_neutral = round((all_group['label_desc'] == 'neutral').mean() * 100, 1)\n",
    "        pct_agree = round((all_group['label_desc'] == 'agree').mean() * 100, 1)\n",
    "        n_unique_comments = all_group['src_comment_id'].nunique() + all_group['dst_comment_id'].nunique()\n",
    "        n_unique_authors = all_group['src_author'].nunique() + all_group['dst_author'].nunique()\n",
    "        stats.insert(0, {\n",
    "            group_col: 'All',\n",
    "            'Earliest Date': earliest,\n",
    "            'Latest Date': latest,\n",
    "            '# Replies': n_replies,\n",
    "            '% Self-Replies': pct_self,\n",
    "            '% Disagree': pct_disagree,\n",
    "            '% Neutral': pct_neutral,\n",
    "            '% Agree': pct_agree,\n",
    "            '# Unique Comments': n_unique_comments,\n",
    "            '# Unique Authors': n_unique_authors\n",
    "        })\n",
    "        print(f\"Basic reply statistics (filtered self-replies):\")\n",
    "        display(pd.DataFrame(stats))\n",
    "\n",
    "    def print_subreddit_timestep_info(df: pd.DataFrame, count_info: str = \"replies\"):\n",
    "        for sid, group in df.groupby('subreddit'):\n",
    "            print(f\"Subreddit: {sid} ({count_info})\")\n",
    "            df_sub = group.groupby(['timestep', 'interval', 'actual_window_size']).size().reset_index(name='total_count')\n",
    "            min_date = pd.to_datetime(group['timestamp'].min()).date()\n",
    "            max_date = pd.to_datetime(group['timestamp'].max()).date()\n",
    "            all_row = {\n",
    "                'timestep': 'All',\n",
    "                'interval': f\"{min_date} - {max_date}\",\n",
    "                'actual_window_size': (max_date - min_date).days,\n",
    "                'total_count': len(group)\n",
    "            }\n",
    "            df_sub = pd.concat([pd.DataFrame([all_row]), df_sub], ignore_index=True)\n",
    "            display(df_sub)\n",
    "    \n",
    "    # Main processor function\n",
    "    def process_data(self, raw_data: pd.DataFrame | None = None, device=None, summarize=True) -> ProcessedData:\n",
    "        if raw_data is None:\n",
    "            raw_data = pd.read_csv(self.raw_path)\n",
    "        else:\n",
    "            if not isinstance(raw_data, pd.DataFrame):\n",
    "                raise ValueError(\"Invalid `raw_data` provided\")\n",
    "                \n",
    "        if not self.processed_path:\n",
    "            os.makedirs(os.path.dirname(self.processed_path), exist_ok=True)\n",
    "\n",
    "        #=========================================================\n",
    "        # BEGIN PROCESSING\n",
    "        #=========================================================\n",
    "        processed_file_names = ['comments.pkl', 'replies.pkl', 'user_pairs.pkl', 'submissions.pkl']\n",
    "        # If processed files already exists, skip processing and load from cache\n",
    "        if all(os.path.exists(os.path.join(self.processed_path, fname)) for fname in processed_file_names):\n",
    "            print(f\"Processed files already exist, loading {processed_file_names} from '{self.processed_path}/'\")            \n",
    "            comments = pd.read_pickle(os.path.join(self.processed_path, 'comments.pkl'))\n",
    "            replies = pd.read_pickle(os.path.join(self.processed_path, 'replies.pkl'))\n",
    "            user_pairs = pd.read_pickle(os.path.join(self.processed_path, 'user_pairs.pkl'))\n",
    "            submissions = pd.read_pickle(os.path.join(self.processed_path, 'submissions.pkl'))\n",
    "            return ProcessedData(comments, replies, user_pairs, submissions)\n",
    "\n",
    "        print(\"Begin Data Processing...\")\n",
    "        cleaned = self.clean_data(raw_data)\n",
    "        comments = self.process_comments(cleaned)\n",
    "        replies = self.process_replies(cleaned, comments)\n",
    "        user_pairs = self.process_user_pairs(replies)\n",
    "        submissions = self.process_submissions(comments)\n",
    "\n",
    "        # Check if configs require text embeddings\n",
    "        embed_comments = self.embedding_cfgs.get('comments', False)\n",
    "        embed_submissions = self.embedding_cfgs.get('submissions', False)\n",
    "        text_emb_cfg = self.embedding_cfgs['model'] if (embed_comments or embed_submissions) else {}\n",
    "        if embed_comments:\n",
    "            print(f\"Embedding comments...\")\n",
    "            comments = self.embed_text_column(device, comments, 'comment_text', text_emb_cfg)\n",
    "        if embed_submissions:\n",
    "            print(f\"Embedding submissions...\")\n",
    "            submissions = self.embed_text_column(device, submissions, 'submission_text', text_emb_cfg)\n",
    "\n",
    "        # Save processed files        \n",
    "        self.save_processed_data('csv', cleaned, f\"{self.processed_path}/deb_label_cleaned.csv\")\n",
    "        print(f\"Saved cleaned data to {self.processed_path}/deb_label_cleaned.csv\")\n",
    "        \n",
    "        self.save_processed_data('pickle', comments, f\"{self.processed_path}/comments.pkl\")\n",
    "        print(f\"Saved processed comments data to {self.processed_path}/comments.pkl\")\n",
    "\n",
    "        self.save_processed_data('pickle', replies, f\"{self.processed_path}/replies.pkl\")\n",
    "        print(f\"Saved processed replies data to {self.processed_path}/replies.pkl\")\n",
    "        \n",
    "        self.save_processed_data('pickle', user_pairs, f\"{self.processed_path}/user_pairs.pkl\")\n",
    "        print(f\"Saved processed user pairs data to {self.processed_path}/user_pairs.pkl\")\n",
    "\n",
    "        self.save_processed_data('pickle', submissions, f\"{self.processed_path}/submissions.pkl\")\n",
    "        print(f\"Saved processed submissions data to {self.processed_path}/submissions.pkl\")\n",
    "\n",
    "        if summarize:\n",
    "            print(\"Basic reply statistics (filtered self-replies):\")\n",
    "            self.print_basic_reply_statistics(replies)\n",
    "            print(\"Subreddit timestep info for replies:\")\n",
    "            self.print_subreddit_timestep_info(replies, count_info=\"replies\")\n",
    "            print(\"Subreddit timestep info for comments:\")\n",
    "            self.print_subreddit_timestep_info(comments, count_info=\"comments\")\n",
    "        return ProcessedData(comments, replies, user_pairs, submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3a1b184d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "Processed files already exist, loading ['comments.pkl', 'replies.pkl', 'user_pairs.pkl', 'submissions.pkl'] from 'data/processed/'\n",
      "replies\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>submission_id</th>\n",
       "      <th>label</th>\n",
       "      <th>label_desc</th>\n",
       "      <th>src_author</th>\n",
       "      <th>src_comment_id</th>\n",
       "      <th>dst_author</th>\n",
       "      <th>dst_comment_id</th>\n",
       "      <th>agreement_fraction</th>\n",
       "      <th>individual_kappa</th>\n",
       "      <th>timestep</th>\n",
       "      <th>interval</th>\n",
       "      <th>actual_window_size</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>republican</td>\n",
       "      <td>2020-12-15 02:12:00</td>\n",
       "      <td>kd0se4</td>\n",
       "      <td>0</td>\n",
       "      <td>disagree</td>\n",
       "      <td>03-Oct</td>\n",
       "      <td>gfvmzei</td>\n",
       "      <td>guildarts15</td>\n",
       "      <td>gfvmv5x</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-10-28 - 2020-12-26</td>\n",
       "      <td>59</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit_id   subreddit           timestamp submission_id  label  \\\n",
       "0             4  republican 2020-12-15 02:12:00        kd0se4      0   \n",
       "\n",
       "  label_desc src_author src_comment_id   dst_author dst_comment_id  \\\n",
       "0   disagree     03-Oct        gfvmzei  guildarts15        gfvmv5x   \n",
       "\n",
       "   agreement_fraction  individual_kappa  timestep                 interval  \\\n",
       "0                 1.0               1.0         5  2020-10-28 - 2020-12-26   \n",
       "\n",
       "   actual_window_size  confidence  \n",
       "0                  59         1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_pairs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>timestep</th>\n",
       "      <th>interval</th>\n",
       "      <th>actual_window_size</th>\n",
       "      <th>src_author</th>\n",
       "      <th>dst_author</th>\n",
       "      <th>mean_agreement_fraction</th>\n",
       "      <th>mean_kappa</th>\n",
       "      <th>mean_confidence</th>\n",
       "      <th>net_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>brexit</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06-21 - 2017-10-13</td>\n",
       "      <td>479</td>\n",
       "      <td>APB2710</td>\n",
       "      <td>EthiczGradient</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit_id subreddit  timestep                 interval  \\\n",
       "0             0    brexit         0  2016-06-21 - 2017-10-13   \n",
       "\n",
       "   actual_window_size src_author      dst_author  mean_agreement_fraction  \\\n",
       "0                 479    APB2710  EthiczGradient                      1.0   \n",
       "\n",
       "   mean_kappa  mean_confidence       net_vector  \n",
       "0         1.0              1.0  [0.0, 1.0, 0.0]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comments\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>submission_id</th>\n",
       "      <th>submission_text</th>\n",
       "      <th>author</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>is_parent</th>\n",
       "      <th>timestep</th>\n",
       "      <th>interval</th>\n",
       "      <th>actual_window_size</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2226</th>\n",
       "      <td>1</td>\n",
       "      <td>blacklivesmatter</td>\n",
       "      <td>2020-05-27 19:47:00</td>\n",
       "      <td>gqy05a</td>\n",
       "      <td>Video shows Minneapolis cop with knee on neck ...</td>\n",
       "      <td>photobarnes</td>\n",
       "      <td>fry0ggg</td>\n",
       "      <td>Time for a revolution. What are we going to do...</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-05-27 - 2020-06-25</td>\n",
       "      <td>29</td>\n",
       "      <td>[0.00779149, 0.06526734, -0.04098327, -0.07576...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddit_id         subreddit           timestamp submission_id  \\\n",
       "2226             1  blacklivesmatter 2020-05-27 19:47:00        gqy05a   \n",
       "\n",
       "                                        submission_text       author  \\\n",
       "2226  Video shows Minneapolis cop with knee on neck ...  photobarnes   \n",
       "\n",
       "     comment_id                                       comment_text  is_parent  \\\n",
       "2226    fry0ggg  Time for a revolution. What are we going to do...       True   \n",
       "\n",
       "      timestep                 interval  actual_window_size  \\\n",
       "2226         0  2020-05-27 - 2020-06-25                  29   \n",
       "\n",
       "                                             embeddings  \n",
       "2226  [0.00779149, 0.06526734, -0.04098327, -0.07576...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submissions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>submission_id</th>\n",
       "      <th>submission_text</th>\n",
       "      <th>first_comment_time</th>\n",
       "      <th>first_comment_timestep</th>\n",
       "      <th>first_comment_interval</th>\n",
       "      <th>first_comment_actual_window_size</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>brexit</td>\n",
       "      <td>4p1sha</td>\n",
       "      <td>Got any leftie friends on the fence, or Pro re...</td>\n",
       "      <td>2016-06-21 15:29:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06-21 - 2017-10-13</td>\n",
       "      <td>479</td>\n",
       "      <td>[0.036245752, -0.05205959, 0.029077008, -0.054...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit_id subreddit submission_id  \\\n",
       "0             0    brexit        4p1sha   \n",
       "\n",
       "                                     submission_text  first_comment_time  \\\n",
       "0  Got any leftie friends on the fence, or Pro re... 2016-06-21 15:29:00   \n",
       "\n",
       "   first_comment_timestep   first_comment_interval  \\\n",
       "0                       0  2016-06-21 - 2017-10-13   \n",
       "\n",
       "   first_comment_actual_window_size  \\\n",
       "0                               479   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [0.036245752, -0.05205959, 0.029077008, -0.054...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_processor = DataProcessor(base_configs)\n",
    "processed_data = data_processor.process_data(device=device)\n",
    "\n",
    "print(\"replies\")\n",
    "display(processed_data.replies.head(1))\n",
    "print(\"user_pairs\")\n",
    "display(processed_data.user_pairs.head(1))\n",
    "print(\"comments\")\n",
    "display(processed_data.comments.head(1))\n",
    "print(\"submissions\")\n",
    "display(processed_data.submissions.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca131ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "Processed files already exist, loading ['comments.pkl', 'replies.pkl', 'user_pairs.pkl', 'submissions.pkl'] from 'data/processed/'\n",
      "replies\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>submission_id</th>\n",
       "      <th>label</th>\n",
       "      <th>label_desc</th>\n",
       "      <th>src_author</th>\n",
       "      <th>src_comment_id</th>\n",
       "      <th>dst_author</th>\n",
       "      <th>dst_comment_id</th>\n",
       "      <th>agreement_fraction</th>\n",
       "      <th>individual_kappa</th>\n",
       "      <th>timestep</th>\n",
       "      <th>interval</th>\n",
       "      <th>actual_window_size</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>republican</td>\n",
       "      <td>2020-12-15 02:12:00</td>\n",
       "      <td>kd0se4</td>\n",
       "      <td>0</td>\n",
       "      <td>disagree</td>\n",
       "      <td>03-Oct</td>\n",
       "      <td>gfvmzei</td>\n",
       "      <td>guildarts15</td>\n",
       "      <td>gfvmv5x</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-10-28 - 2020-12-26</td>\n",
       "      <td>59</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit_id   subreddit           timestamp submission_id  label  \\\n",
       "0             4  republican 2020-12-15 02:12:00        kd0se4      0   \n",
       "\n",
       "  label_desc src_author src_comment_id   dst_author dst_comment_id  \\\n",
       "0   disagree     03-Oct        gfvmzei  guildarts15        gfvmv5x   \n",
       "\n",
       "   agreement_fraction  individual_kappa  timestep                 interval  \\\n",
       "0                 1.0               1.0         5  2020-10-28 - 2020-12-26   \n",
       "\n",
       "   actual_window_size  confidence  \n",
       "0                  59         1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_pairs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>timestep</th>\n",
       "      <th>interval</th>\n",
       "      <th>actual_window_size</th>\n",
       "      <th>src_author</th>\n",
       "      <th>dst_author</th>\n",
       "      <th>mean_agreement_fraction</th>\n",
       "      <th>mean_kappa</th>\n",
       "      <th>mean_confidence</th>\n",
       "      <th>net_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>brexit</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06-21 - 2017-10-13</td>\n",
       "      <td>479</td>\n",
       "      <td>APB2710</td>\n",
       "      <td>EthiczGradient</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit_id subreddit  timestep                 interval  \\\n",
       "0             0    brexit         0  2016-06-21 - 2017-10-13   \n",
       "\n",
       "   actual_window_size src_author      dst_author  mean_agreement_fraction  \\\n",
       "0                 479    APB2710  EthiczGradient                      1.0   \n",
       "\n",
       "   mean_kappa  mean_confidence       net_vector  \n",
       "0         1.0              1.0  [0.0, 1.0, 0.0]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comments\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>submission_id</th>\n",
       "      <th>submission_text</th>\n",
       "      <th>author</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>is_parent</th>\n",
       "      <th>timestep</th>\n",
       "      <th>interval</th>\n",
       "      <th>actual_window_size</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2226</th>\n",
       "      <td>1</td>\n",
       "      <td>blacklivesmatter</td>\n",
       "      <td>2020-05-27 19:47:00</td>\n",
       "      <td>gqy05a</td>\n",
       "      <td>Video shows Minneapolis cop with knee on neck ...</td>\n",
       "      <td>photobarnes</td>\n",
       "      <td>fry0ggg</td>\n",
       "      <td>Time for a revolution. What are we going to do...</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-05-27 - 2020-06-25</td>\n",
       "      <td>29</td>\n",
       "      <td>[0.00779149, 0.06526734, -0.04098327, -0.07576...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddit_id         subreddit           timestamp submission_id  \\\n",
       "2226             1  blacklivesmatter 2020-05-27 19:47:00        gqy05a   \n",
       "\n",
       "                                        submission_text       author  \\\n",
       "2226  Video shows Minneapolis cop with knee on neck ...  photobarnes   \n",
       "\n",
       "     comment_id                                       comment_text  is_parent  \\\n",
       "2226    fry0ggg  Time for a revolution. What are we going to do...       True   \n",
       "\n",
       "      timestep                 interval  actual_window_size  \\\n",
       "2226         0  2020-05-27 - 2020-06-25                  29   \n",
       "\n",
       "                                             embeddings  \n",
       "2226  [0.00779149, 0.06526734, -0.04098327, -0.07576...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submissions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>submission_id</th>\n",
       "      <th>submission_text</th>\n",
       "      <th>first_comment_time</th>\n",
       "      <th>first_comment_timestep</th>\n",
       "      <th>first_comment_interval</th>\n",
       "      <th>first_comment_actual_window_size</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>brexit</td>\n",
       "      <td>4p1sha</td>\n",
       "      <td>Got any leftie friends on the fence, or Pro re...</td>\n",
       "      <td>2016-06-21 15:29:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06-21 - 2017-10-13</td>\n",
       "      <td>479</td>\n",
       "      <td>[0.036245752, -0.05205959, 0.029077008, -0.054...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit_id subreddit submission_id  \\\n",
       "0             0    brexit        4p1sha   \n",
       "\n",
       "                                     submission_text  first_comment_time  \\\n",
       "0  Got any leftie friends on the fence, or Pro re... 2016-06-21 15:29:00   \n",
       "\n",
       "   first_comment_timestep   first_comment_interval  \\\n",
       "0                       0  2016-06-21 - 2017-10-13   \n",
       "\n",
       "   first_comment_actual_window_size  \\\n",
       "0                               479   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [0.036245752, -0.05205959, 0.029077008, -0.054...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# try import from src.modules.data_processor\n",
    "\n",
    "from src.modules.data_processor import DataProcessor\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_processor = DataProcessor(base_configs)\n",
    "processed_data = data_processor.process_data(device=device)\n",
    "print(\"replies\")\n",
    "display(processed_data.replies.head(1))\n",
    "print(\"user_pairs\")\n",
    "display(processed_data.user_pairs.head(1))\n",
    "print(\"comments\")\n",
    "display(processed_data.comments.head(1))\n",
    "print(\"submissions\")\n",
    "display(processed_data.submissions.head(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mscenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
