{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df46122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload package\n",
    "import importlib\n",
    "import src.utils.config_loader\n",
    "importlib.reload(src.utils.config_loader)\n",
    "\n",
    "from src.utils.config_loader import ConfigLoader\n",
    "\n",
    "config_loader = ConfigLoader()\n",
    "all_configs = config_loader.load_configs()\n",
    "\n",
    "base_configs = config_loader.get_section(all_configs, \"base\")\n",
    "graph_configs = config_loader.get_section(all_configs, \"graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7be83402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "Processed files already exist, loading ['comments.pkl', 'replies.pkl', 'user_pairs.pkl', 'submissions.pkl'] from 'data/processed/'\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "from src.modules.data_processor import DataProcessor\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_processor = DataProcessor(base_configs=base_configs)\n",
    "processed_data = data_processor.process_data(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f559229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building node features with pooling: mean\n",
      "    + Total unique authors in pairs: 35257\n",
      "    + Total pooled vectors: 35212\n",
      "    + Pooled vector dimension: 384\n",
      "Building graph snapshots: directed=True, use_wcc=True, edge_attrs=['mean_confidence', 'net_vector']\n",
      "    + [Subreddit 0, T0] 2 edges filtered by WCC (86 -> 84)\n",
      "    + [Subreddit 0, T7] 1 edges filtered by WCC (177 -> 176)\n",
      "    + [Subreddit 0, T11] 1 edges filtered by WCC (944 -> 943)\n",
      "    + [Subreddit 0, T13] 4 edges filtered by WCC (701 -> 697)\n",
      "    + [Subreddit 0, T15] 1 edges filtered by WCC (1195 -> 1194)\n",
      "    + [Subreddit 0, T16] 2 edges filtered by WCC (608 -> 606)\n",
      "    + [Subreddit 0, T18] 1 edges filtered by WCC (948 -> 947)\n",
      "    + [Subreddit 0, T19] 1 edges filtered by WCC (893 -> 892)\n",
      "    + [Subreddit 0, T21] 2 edges filtered by WCC (544 -> 542)\n",
      "    + [Subreddit 0, T22] 4 edges filtered by WCC (421 -> 417)\n",
      "    + [Subreddit 1, T0] 456 edges filtered by WCC (599 -> 143)\n",
      "    + [Subreddit 1, T1] 281 edges filtered by WCC (323 -> 42)\n",
      "    + [Subreddit 1, T2] 164 edges filtered by WCC (171 -> 7)\n",
      "    + [Subreddit 1, T3] 155 edges filtered by WCC (180 -> 25)\n",
      "    + [Subreddit 1, T4] 154 edges filtered by WCC (165 -> 11)\n",
      "    + [Subreddit 1, T5] 117 edges filtered by WCC (139 -> 22)\n",
      "    + [Subreddit 1, T6] 63 edges filtered by WCC (69 -> 6)\n",
      "    + [Subreddit 1, T7] 73 edges filtered by WCC (77 -> 4)\n",
      "    + [Subreddit 1, T8] 55 edges filtered by WCC (61 -> 6)\n",
      "    + [Subreddit 1, T9] 99 edges filtered by WCC (110 -> 11)\n",
      "    + [Subreddit 2, T0] 32 edges filtered by WCC (119 -> 87)\n",
      "    + [Subreddit 2, T1] 27 edges filtered by WCC (110 -> 83)\n",
      "    + [Subreddit 2, T2] 30 edges filtered by WCC (125 -> 95)\n",
      "    + [Subreddit 2, T3] 37 edges filtered by WCC (111 -> 74)\n",
      "    + [Subreddit 2, T4] 31 edges filtered by WCC (111 -> 80)\n",
      "    + [Subreddit 2, T5] 34 edges filtered by WCC (100 -> 66)\n",
      "    + [Subreddit 2, T6] 58 edges filtered by WCC (121 -> 63)\n",
      "    + [Subreddit 2, T7] 86 edges filtered by WCC (222 -> 136)\n",
      "    + [Subreddit 2, T8] 95 edges filtered by WCC (149 -> 54)\n",
      "    + [Subreddit 2, T9] 71 edges filtered by WCC (95 -> 24)\n",
      "    + [Subreddit 2, T10] 48 edges filtered by WCC (105 -> 57)\n",
      "    + [Subreddit 2, T11] 41 edges filtered by WCC (57 -> 16)\n",
      "    + [Subreddit 2, T12] 76 edges filtered by WCC (97 -> 21)\n",
      "    + [Subreddit 2, T13] 62 edges filtered by WCC (82 -> 20)\n",
      "    + [Subreddit 2, T14] 105 edges filtered by WCC (133 -> 28)\n",
      "    + [Subreddit 2, T15] 133 edges filtered by WCC (224 -> 91)\n",
      "    + [Subreddit 2, T16] 96 edges filtered by WCC (236 -> 140)\n",
      "    + [Subreddit 2, T17] 134 edges filtered by WCC (433 -> 299)\n",
      "    + [Subreddit 2, T18] 190 edges filtered by WCC (562 -> 372)\n",
      "    + [Subreddit 2, T19] 128 edges filtered by WCC (535 -> 407)\n",
      "    + [Subreddit 2, T20] 196 edges filtered by WCC (466 -> 270)\n",
      "    + [Subreddit 2, T21] 174 edges filtered by WCC (297 -> 123)\n",
      "    + [Subreddit 2, T22] 115 edges filtered by WCC (320 -> 205)\n",
      "    + [Subreddit 2, T23] 149 edges filtered by WCC (400 -> 251)\n",
      "    + [Subreddit 2, T24] 137 edges filtered by WCC (262 -> 125)\n",
      "    + [Subreddit 2, T25] 115 edges filtered by WCC (236 -> 121)\n",
      "    + [Subreddit 3, T0] 210 edges filtered by WCC (1116 -> 906)\n",
      "    + [Subreddit 3, T1] 208 edges filtered by WCC (1410 -> 1202)\n",
      "    + [Subreddit 3, T2] 174 edges filtered by WCC (1440 -> 1266)\n",
      "    + [Subreddit 3, T3] 158 edges filtered by WCC (1102 -> 944)\n",
      "    + [Subreddit 3, T4] 315 edges filtered by WCC (1103 -> 788)\n",
      "    + [Subreddit 3, T5] 342 edges filtered by WCC (1356 -> 1014)\n",
      "    + [Subreddit 3, T6] 275 edges filtered by WCC (951 -> 676)\n",
      "    + [Subreddit 3, T7] 187 edges filtered by WCC (659 -> 472)\n",
      "    + [Subreddit 3, T8] 153 edges filtered by WCC (378 -> 225)\n",
      "    + [Subreddit 4, T0] 189 edges filtered by WCC (465 -> 276)\n",
      "    + [Subreddit 4, T1] 342 edges filtered by WCC (760 -> 418)\n",
      "    + [Subreddit 4, T2] 429 edges filtered by WCC (1152 -> 723)\n",
      "    + [Subreddit 4, T3] 460 edges filtered by WCC (1368 -> 908)\n",
      "    + [Subreddit 4, T4] 607 edges filtered by WCC (1680 -> 1073)\n",
      "    + [Subreddit 4, T5] 564 edges filtered by WCC (1896 -> 1332)\n",
      "    + [Subreddit 4, T6] 433 edges filtered by WCC (833 -> 400)\n",
      "    + [Subreddit 4, T7] 286 edges filtered by WCC (1013 -> 727)\n",
      "    + [Subreddit 4, T8] 180 edges filtered by WCC (549 -> 369)\n",
      "Converting NetworkX graphs to PyG data objects\n",
      "    + Created 77 PyG graphs\n",
      "Saved PyG graphs to data/processed/pyg_graphs_384D.pt\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from typing import Dict, Any, NamedTuple, Tuple\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "import os\n",
    "\n",
    "class ProcessedData(NamedTuple):\n",
    "    node_dict: Dict[int, Dict[int, Dict[str, Any]]]\n",
    "    graph_dict: Dict[int, Dict[int, nx.Graph]]\n",
    "    pyg_graphs: list[Data]\n",
    "    pyg_node_map: Dict[Tuple[int, int], Dict[str, int]]\n",
    "\n",
    "class GraphProcessor:\n",
    "    def __init__(self, graph_configs, processed_path: str = \"data/processed\"):\n",
    "        self.processed_path = processed_path\n",
    "        self.graph_configs = graph_configs\n",
    "        self.graphs = {}\n",
    "    \n",
    "    def build_node_features(self, embeddings_source: pd.DataFrame, pairs: pd.DataFrame, configs: Dict[str, Any]) -> Tuple[Dict[int, Dict[int, Dict[str, np.ndarray]]], int]:\n",
    "        \"\"\"Pool embeddings per (subreddit_id, timestep, author) from [comments] data\"\"\"\n",
    "        def _pool(arrs):\n",
    "            arrs = np.stack(arrs, axis=0)\n",
    "            if pooling == 'mean':\n",
    "                return arrs.mean(axis=0)\n",
    "            elif pooling == 'sum':\n",
    "                return arrs.sum(axis=0)\n",
    "            elif pooling == 'max':\n",
    "                return arrs.max(axis=0)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported pooling: {pooling}\")\n",
    "        \n",
    "        pooling = configs.get('pooling', 'mean')\n",
    "        print(f\"Building node features with pooling: {pooling}\")\n",
    "        \n",
    "        # 1. Collect all authors from user pairs\n",
    "        all_users = pd.concat([\n",
    "            pairs[['subreddit_id','timestep','src_author']].rename(columns={'src_author': 'author'}),\n",
    "            pairs[['subreddit_id','timestep','dst_author']].rename(columns={'dst_author': 'author'})\n",
    "        ], ignore_index=True).drop_duplicates()\n",
    "        print(f\"    + Total unique authors in pairs: {len(all_users)}\")\n",
    "\n",
    "        # 2. Filter embeddings to only those authors & snapshots\n",
    "        embs_df = embeddings_source[['subreddit_id','timestep','author','embeddings']]\n",
    "        embs_df = embs_df.merge(all_users, on=['subreddit_id','timestep','author'], how='inner')\n",
    "        embs_df['embeddings'] = embs_df['embeddings'].map(\n",
    "            lambda x: x if isinstance(x, np.ndarray) else np.asarray(x, dtype=float)\n",
    "        )\n",
    "\n",
    "        # 3. Pool within each (subreddit, timestep, author)\n",
    "        pooled = embs_df.groupby(['subreddit_id','timestep','author'], sort=False)['embeddings'].agg(_pool)\n",
    "        emb_dim = pooled.iloc[0].shape[0] if not pooled.empty else 0\n",
    "        print(f\"    + Total pooled vectors: {len(pooled)}\")\n",
    "        print(f\"    + Pooled vector dimension: {emb_dim}\")\n",
    "\n",
    "        # 4. Convert to nested dict\n",
    "        node_dict: Dict[int, Dict[int, Dict[str, np.ndarray]]] = defaultdict(lambda: defaultdict(dict))\n",
    "        for index, vec in pooled.items():\n",
    "            sub, ts, author = index     # type: ignore\n",
    "            node_dict[int(sub)][int(ts)][str(author)] = vec\n",
    "        \n",
    "        return {sub: dict(ts_dict) for sub, ts_dict in node_dict.items()}, emb_dim\n",
    "    \n",
    "    def build_graph_snapshots(self, pairs: pd.DataFrame, node_dict: Dict[int, Dict[int, Dict[str, np.ndarray]]], configs: Dict[str, Any]) -> Dict[int, Dict[int, nx.Graph]]:\n",
    "        # {subreddit_id: {timestep: graph}}\n",
    "        directed = configs.get('directed', True)\n",
    "        use_wcc = configs.get('use_wcc', False)\n",
    "        edge_attrs = configs.get('edge_attrs', ['mean_confidence', 'net_vector'])\n",
    "        print(f\"Building graph snapshots: directed={directed}, use_wcc={use_wcc}, edge_attrs={edge_attrs}\")\n",
    "        graph_dict = {}\n",
    "        \n",
    "        # Build from pairs\n",
    "        for (subreddit_id, timestep), group in pairs.groupby(['subreddit_id', 'timestep']):\n",
    "            subreddit_id = int(subreddit_id)\n",
    "            timestep = int(timestep)\n",
    "\n",
    "            G = nx.DiGraph() if directed else nx.Graph()\n",
    "\n",
    "            # Add nodes with features\n",
    "            ts_nodes = node_dict.get(subreddit_id, {}).get(timestep, {})\n",
    "            for author, embedding in ts_nodes.items():\n",
    "                G.add_node(author, embedding=embedding)\n",
    "\n",
    "            # Add edges\n",
    "            for _, row in group.iterrows():\n",
    "                src = row['src_author']\n",
    "                dst = row['dst_author']\n",
    "                if src in G and dst in G:\n",
    "                    edge_data = {attr: row[attr] for attr in edge_attrs if attr in row}\n",
    "                    G.add_edge(src, dst, **edge_data)\n",
    "            \n",
    "            # Apply weakly connected component filtering if requested\n",
    "            if use_wcc and len(G.nodes()) > 0:\n",
    "                edges_before_wcc = len(G.edges())\n",
    "                if directed:\n",
    "                    # For directed graphs, get largest weakly connected component\n",
    "                    wcc_components = list(nx.weakly_connected_components(G))\n",
    "                else:\n",
    "                    # For undirected graphs, get largest connected component\n",
    "                    wcc_components = list(nx.connected_components(G))\n",
    "                \n",
    "                if wcc_components:\n",
    "                    # Get the largest component\n",
    "                    largest_component = max(wcc_components, key=len)\n",
    "                    G = G.subgraph(largest_component).copy()\n",
    "                    \n",
    "                    # Log filtered\n",
    "                    edges_after_wcc = len(G.edges())\n",
    "                    edges_filtered = edges_before_wcc - edges_after_wcc\n",
    "                    if edges_filtered > 0:\n",
    "                        print(f\"    + [Subreddit {subreddit_id}, T{timestep}] {edges_filtered} edges filtered by WCC ({edges_before_wcc} -> {edges_after_wcc})\")\n",
    "            \n",
    "            graph_dict.setdefault(subreddit_id, {})[timestep] = G\n",
    "\n",
    "        return graph_dict\n",
    "\n",
    "    def build_pyg_graphs(self, graphs_dict) -> tuple[list, dict]:\n",
    "        \"\"\"Convert NetworkX graphs to PyG Data objects\"\"\"\n",
    "        print(\"Converting NetworkX graphs to PyG data objects\")\n",
    "        pyg_graphs = []\n",
    "        master_node_map = {}  # {(subreddit_id, timestep): {node_name: idx}}\n",
    "        \n",
    "        # Sort by subreddit_id and timestep for consistent ordering\n",
    "        for sub in sorted(graphs_dict.keys()):\n",
    "            ts_dict = graphs_dict[sub]\n",
    "            for ts in sorted(ts_dict.keys()):\n",
    "                G = ts_dict[ts]\n",
    "                if len(G.nodes()) == 0:\n",
    "                    continue    # Skip empty graphs\n",
    "                \n",
    "                node_list = sorted(G.nodes())\n",
    "                node_map = {node: idx for idx, node in enumerate(node_list)}\n",
    "                master_node_map[(sub, ts)] = node_map\n",
    "\n",
    "                # Extract node features\n",
    "                node_features = []\n",
    "                for node in node_list:\n",
    "                    embedding = G.nodes[node].get('embedding')\n",
    "                    if embedding is not None:\n",
    "                        node_features.append(embedding)\n",
    "                    else:\n",
    "                        node_features.append(np.zeros(384))  # Default to zero vector if no embedding\n",
    "                x = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "                # Extract edges\n",
    "                edge_list, edge_attrs = [], []\n",
    "                for src, dst, data in G.edges(data=True):\n",
    "                    src_idx = node_map[src]\n",
    "                    dst_idx = node_map[dst]\n",
    "                    edge_list.append([src_idx, dst_idx])\n",
    "\n",
    "                    # Extract edge attributes\n",
    "                    edge_attr = []\n",
    "                    for attr in ['mean_confidence', 'net_vector']:\n",
    "                        if attr in data:\n",
    "                            if attr == 'net_vector' and isinstance(data[attr], (list, np.ndarray)):\n",
    "                                edge_attr.extend(data[attr])  # Flatten vector attributes\n",
    "                            else:\n",
    "                                edge_attr.append(data[attr])\n",
    "                    edge_attrs.append(edge_attr)\n",
    "            \n",
    "                # Convert to tensors (moved inside the timestep loop)\n",
    "                if edge_list:\n",
    "                    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "                    edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n",
    "                else:\n",
    "                    edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "                    edge_attr = torch.empty((0, len(['mean_confidence', 'net_vector'])), dtype=torch.float)\n",
    "                \n",
    "                # Create PyG data object (moved inside the timestep loop)\n",
    "                data = Data(\n",
    "                    x=x,\n",
    "                    edge_index=edge_index,\n",
    "                    edge_attr=edge_attr,\n",
    "                    num_nodes=len(node_list),\n",
    "                    num_edges=len(edge_list),\n",
    "                    node_map=node_map,\n",
    "                    subreddit_id=sub,\n",
    "                    local_timestep=ts\n",
    "                )\n",
    "                pyg_graphs.append(data)\n",
    "        \n",
    "        print(f\"    + Created {len(pyg_graphs)} PyG graphs\")\n",
    "        return pyg_graphs, master_node_map\n",
    "\n",
    "    def process_data(self, pairs: pd.DataFrame, embeddings_source: pd.DataFrame) -> ProcessedData:\n",
    "        \"\"\"Process data to create node features and graph snapshots\"\"\"\n",
    "        # 1. Build node features\n",
    "        node_cfg = self.graph_configs.get('node_features', {})\n",
    "        node_dict, vec_dim = self.build_node_features(embeddings_source, pairs, node_cfg)\n",
    "        \n",
    "        # 2. Build graph snapshots\n",
    "        construction_cfg = self.graph_configs.get('construction', {})\n",
    "        graph_dict = self.build_graph_snapshots(pairs, node_dict, construction_cfg)\n",
    "\n",
    "        # 3. Build PyG graphs\n",
    "        pyg_graphs, pyg_node_map = self.build_pyg_graphs(graph_dict)\n",
    "        if not os.path.exists(self.processed_path):\n",
    "            os.makedirs(self.processed_path)\n",
    "\n",
    "        # 4. Save PyG graphs to file\n",
    "        torch.save(pyg_graphs, os.path.join(self.processed_path, f'pyg_graphs_{vec_dim}D.pt'))\n",
    "        print(f\"Saved PyG graphs to {self.processed_path}/pyg_graphs_{vec_dim}D.pt\")\n",
    "        \n",
    "        return ProcessedData(\n",
    "            node_dict=node_dict, \n",
    "            graph_dict=graph_dict, \n",
    "            pyg_graphs=pyg_graphs,\n",
    "            pyg_node_map=pyg_node_map\n",
    "        )\n",
    "\n",
    "# Graph snapshots\n",
    "pairs = processed_data.user_pairs\n",
    "comments = processed_data.comments\n",
    "processed_path = base_configs.get('processed_path', 'data/processed')\n",
    "graph_processor = GraphProcessor(graph_configs)\n",
    "graph_data = graph_processor.process_data(pairs, comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f2a55f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample graph: 29 nodes, 84 edges\n",
      "Found 3 communities\n",
      "Modularity: 0.250\n",
      "Community 2: 9 members: ['APB2710', 'Petemcfuzzbuzz', 'StrixTechnica', 'amgiecorker', 'pikadrew']...\n",
      "Community 1: 9 members: ['AnomalyNexus', 'EthiczGradient', 'Heruss100', 'Prituh', 'ScarletIT']...\n",
      "Community 0: 11 members: ['ArchbishopMegatronQC', 'Bozata1', 'Greengoblingogo', 'MrPuddington2', 'SideburnsOfDoom']...\n"
     ]
    }
   ],
   "source": [
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import networkx as nx\n",
    "\n",
    "# Convert NetworkX graph to igraph for Leiden algorithm\n",
    "def nx_to_igraph(G):\n",
    "    \"\"\"Convert NetworkX graph to igraph\"\"\"\n",
    "    # Create igraph from edge list\n",
    "    edges = list(G.edges())\n",
    "    nodes = sorted(G.nodes())\n",
    "    \n",
    "    # Create igraph graph\n",
    "    ig_graph = ig.Graph()\n",
    "    ig_graph.add_vertices(len(nodes))\n",
    "    \n",
    "    # Map node names to indices\n",
    "    node_to_idx = {node: i for i, node in enumerate(nodes)}\n",
    "    idx_to_node = {i: node for i, node in enumerate(nodes)}\n",
    "    \n",
    "    # Add edges using indices\n",
    "    edge_indices = [(node_to_idx[src], node_to_idx[dst]) for src, dst in edges]\n",
    "    ig_graph.add_edges(edge_indices)\n",
    "    \n",
    "    # Set vertex names\n",
    "    ig_graph.vs['name'] = nodes\n",
    "    \n",
    "    return ig_graph, node_to_idx, idx_to_node\n",
    "\n",
    "# Run Leiden community detection\n",
    "sample_graph = graph_data.graph_dict[0][0]\n",
    "print(f\"Sample graph: {len(sample_graph.nodes())} nodes, {len(sample_graph.edges())} edges\")\n",
    "\n",
    "# Convert to igraph\n",
    "ig_graph, node_to_idx, idx_to_node = nx_to_igraph(sample_graph)\n",
    "\n",
    "# Run Leiden algorithm\n",
    "partition = la.find_partition(ig_graph, la.ModularityVertexPartition, seed=42)\n",
    "\n",
    "print(f\"Found {len(partition)} communities\")\n",
    "print(f\"Modularity: {partition.modularity:.3f}\")\n",
    "\n",
    "# Show community assignments\n",
    "communities = {}\n",
    "for i, community_id in enumerate(partition.membership):\n",
    "    node_name = idx_to_node[i]\n",
    "    communities.setdefault(community_id, []).append(node_name)\n",
    "\n",
    "for comm_id, members in communities.items():\n",
    "    print(f\"Community {comm_id}: {len(members)} members: {members[:5]}{'...' if len(members) > 5 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcc0c687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Community Detection Analysis ===\n",
      "Sub 0, TS 0: 29 nodes, 84 edges, 3 communities, modularity: 0.250\n",
      "Sub 0, TS 1: 30 nodes, 79 edges, 4 communities, modularity: 0.327\n",
      "Sub 0, TS 2: 31 nodes, 59 edges, 6 communities, modularity: 0.402\n",
      "Sub 0, TS 3: 43 nodes, 107 edges, 6 communities, modularity: 0.313\n",
      "Sub 0, TS 4: 53 nodes, 134 edges, 6 communities, modularity: 0.337\n",
      "Sub 0, TS 5: 71 nodes, 266 edges, 4 communities, modularity: 0.288\n",
      "Sub 0, TS 6: 79 nodes, 187 edges, 6 communities, modularity: 0.405\n",
      "Sub 0, TS 7: 94 nodes, 176 edges, 8 communities, modularity: 0.468\n",
      "Sub 0, TS 8: 187 nodes, 611 edges, 9 communities, modularity: 0.350\n",
      "Sub 0, TS 9: 271 nodes, 1090 edges, 9 communities, modularity: 0.317\n",
      "Sub 0, TS 10: 214 nodes, 555 edges, 9 communities, modularity: 0.402\n",
      "Sub 0, TS 11: 261 nodes, 943 edges, 9 communities, modularity: 0.325\n",
      "Sub 0, TS 12: 330 nodes, 1414 edges, 10 communities, modularity: 0.292\n",
      "Sub 0, TS 13: 265 nodes, 697 edges, 11 communities, modularity: 0.414\n",
      "Sub 0, TS 14: 307 nodes, 1196 edges, 10 communities, modularity: 0.304\n",
      "Sub 0, TS 15: 291 nodes, 1194 edges, 9 communities, modularity: 0.304\n",
      "Sub 0, TS 16: 206 nodes, 606 edges, 9 communities, modularity: 0.365\n",
      "Sub 0, TS 17: 257 nodes, 1007 edges, 9 communities, modularity: 0.304\n",
      "Sub 0, TS 18: 264 nodes, 947 edges, 9 communities, modularity: 0.340\n",
      "Sub 0, TS 19: 265 nodes, 892 edges, 10 communities, modularity: 0.343\n",
      "Sub 0, TS 20: 331 nodes, 1377 edges, 11 communities, modularity: 0.309\n",
      "Sub 0, TS 21: 221 nodes, 542 edges, 9 communities, modularity: 0.432\n",
      "Sub 0, TS 22: 186 nodes, 417 edges, 9 communities, modularity: 0.450\n",
      "Sub 1, TS 0: 144 nodes, 143 edges, 13 communities, modularity: 0.815\n",
      "Sub 1, TS 1: 43 nodes, 42 edges, 7 communities, modularity: 0.701\n",
      "Sub 1, TS 2: 8 nodes, 7 edges, 3 communities, modularity: 0.357\n",
      "Sub 1, TS 3: 26 nodes, 25 edges, 5 communities, modularity: 0.619\n",
      "Sub 1, TS 4: 12 nodes, 11 edges, 3 communities, modularity: 0.426\n",
      "Sub 1, TS 5: 23 nodes, 22 edges, 5 communities, modularity: 0.612\n",
      "Sub 1, TS 6: 7 nodes, 6 edges, 3 communities, modularity: 0.319\n",
      "Sub 1, TS 7: 5 nodes, 4 edges, 2 communities, modularity: 0.219\n",
      "Sub 1, TS 8: 7 nodes, 6 edges, 2 communities, modularity: 0.208\n",
      "Sub 1, TS 9: 12 nodes, 11 edges, 3 communities, modularity: 0.483\n",
      "Sub 2, TS 0: 70 nodes, 87 edges, 7 communities, modularity: 0.635\n",
      "Sub 2, TS 1: 73 nodes, 83 edges, 8 communities, modularity: 0.654\n",
      "Sub 2, TS 2: 88 nodes, 95 edges, 10 communities, modularity: 0.737\n",
      "Sub 2, TS 3: 73 nodes, 74 edges, 9 communities, modularity: 0.759\n",
      "Sub 2, TS 4: 72 nodes, 80 edges, 8 communities, modularity: 0.710\n",
      "Sub 2, TS 5: 62 nodes, 66 edges, 7 communities, modularity: 0.692\n",
      "Sub 2, TS 6: 64 nodes, 63 edges, 7 communities, modularity: 0.760\n",
      "Sub 2, TS 7: 133 nodes, 136 edges, 11 communities, modularity: 0.821\n",
      "Sub 2, TS 8: 54 nodes, 54 edges, 7 communities, modularity: 0.708\n",
      "Sub 2, TS 9: 24 nodes, 24 edges, 4 communities, modularity: 0.622\n",
      "Sub 2, TS 10: 54 nodes, 57 edges, 7 communities, modularity: 0.657\n",
      "Sub 2, TS 11: 17 nodes, 16 edges, 4 communities, modularity: 0.551\n",
      "Sub 2, TS 12: 22 nodes, 21 edges, 4 communities, modularity: 0.592\n",
      "Sub 2, TS 13: 21 nodes, 20 edges, 5 communities, modularity: 0.585\n",
      "Sub 2, TS 14: 29 nodes, 28 edges, 5 communities, modularity: 0.638\n",
      "Sub 2, TS 15: 86 nodes, 91 edges, 9 communities, modularity: 0.718\n",
      "Sub 2, TS 16: 132 nodes, 140 edges, 11 communities, modularity: 0.767\n",
      "Sub 2, TS 17: 269 nodes, 299 edges, 16 communities, modularity: 0.787\n",
      "Sub 2, TS 18: 341 nodes, 372 edges, 18 communities, modularity: 0.819\n",
      "Sub 2, TS 19: 359 nodes, 407 edges, 19 communities, modularity: 0.805\n",
      "Sub 2, TS 20: 254 nodes, 270 edges, 14 communities, modularity: 0.807\n",
      "Sub 2, TS 21: 121 nodes, 123 edges, 11 communities, modularity: 0.760\n",
      "Sub 2, TS 22: 197 nodes, 205 edges, 13 communities, modularity: 0.807\n",
      "Sub 2, TS 23: 243 nodes, 251 edges, 17 communities, modularity: 0.831\n",
      "Sub 2, TS 24: 123 nodes, 125 edges, 10 communities, modularity: 0.793\n",
      "Sub 2, TS 25: 117 nodes, 121 edges, 11 communities, modularity: 0.778\n",
      "Sub 3, TS 0: 816 nodes, 906 edges, 26 communities, modularity: 0.843\n",
      "Sub 3, TS 1: 965 nodes, 1202 edges, 26 communities, modularity: 0.770\n",
      "Sub 3, TS 2: 1044 nodes, 1266 edges, 28 communities, modularity: 0.793\n",
      "Sub 3, TS 3: 791 nodes, 944 edges, 25 communities, modularity: 0.792\n",
      "Sub 3, TS 4: 716 nodes, 788 edges, 24 communities, modularity: 0.849\n",
      "Sub 3, TS 5: 911 nodes, 1014 edges, 31 communities, modularity: 0.846\n",
      "Sub 3, TS 6: 608 nodes, 676 edges, 24 communities, modularity: 0.828\n",
      "Sub 3, TS 7: 424 nodes, 472 edges, 21 communities, modularity: 0.811\n",
      "Sub 3, TS 8: 217 nodes, 225 edges, 16 communities, modularity: 0.837\n",
      "Sub 4, TS 0: 259 nodes, 276 edges, 17 communities, modularity: 0.815\n",
      "Sub 4, TS 1: 401 nodes, 418 edges, 19 communities, modularity: 0.865\n",
      "Sub 4, TS 2: 663 nodes, 723 edges, 22 communities, modularity: 0.845\n",
      "Sub 4, TS 3: 854 nodes, 908 edges, 28 communities, modularity: 0.880\n",
      "Sub 4, TS 4: 975 nodes, 1073 edges, 30 communities, modularity: 0.852\n",
      "Sub 4, TS 5: 1208 nodes, 1332 edges, 32 communities, modularity: 0.859\n",
      "Sub 4, TS 6: 385 nodes, 400 edges, 19 communities, modularity: 0.868\n",
      "Sub 4, TS 7: 679 nodes, 727 edges, 24 communities, modularity: 0.873\n",
      "Sub 4, TS 8: 336 nodes, 369 edges, 17 communities, modularity: 0.812\n",
      "\n",
      "=== Summary Statistics ===\n",
      "Total graphs analyzed: 77\n",
      "Average modularity: 0.606\n",
      "Average communities per graph: 11.8\n",
      "Modularity range: 0.208 - 0.880\n",
      "\n",
      "=== Detailed Results ===\n",
      "Sub 0, TS 0: Communities: 3, Mod: 0.250, Sizes: [11, 9, 9]\n",
      "Sub 0, TS 1: Communities: 4, Mod: 0.327, Sizes: [9, 9, 6, 6]\n",
      "Sub 0, TS 2: Communities: 6, Mod: 0.402, Sizes: [7, 5, 5, 5, 5...]\n",
      "Sub 0, TS 3: Communities: 6, Mod: 0.313, Sizes: [12, 10, 7, 7, 4...]\n",
      "Sub 0, TS 4: Communities: 6, Mod: 0.337, Sizes: [12, 11, 9, 8, 7...]\n",
      "Sub 0, TS 5: Communities: 4, Mod: 0.288, Sizes: [23, 17, 16, 15]\n",
      "Sub 0, TS 6: Communities: 6, Mod: 0.405, Sizes: [17, 15, 14, 14, 11...]\n",
      "Sub 0, TS 7: Communities: 8, Mod: 0.468, Sizes: [16, 15, 13, 13, 11...]\n",
      "Sub 0, TS 8: Communities: 9, Mod: 0.350, Sizes: [30, 26, 26, 22, 17...]\n",
      "Sub 0, TS 9: Communities: 9, Mod: 0.317, Sizes: [45, 41, 41, 34, 26...]\n",
      "Sub 0, TS 10: Communities: 9, Mod: 0.402, Sizes: [30, 29, 27, 26, 26...]\n",
      "Sub 0, TS 11: Communities: 9, Mod: 0.325, Sizes: [46, 36, 34, 30, 29...]\n",
      "Sub 0, TS 12: Communities: 10, Mod: 0.292, Sizes: [43, 43, 42, 39, 32...]\n",
      "Sub 0, TS 13: Communities: 11, Mod: 0.414, Sizes: [33, 30, 29, 27, 26...]\n",
      "Sub 0, TS 14: Communities: 10, Mod: 0.304, Sizes: [42, 39, 38, 37, 33...]\n",
      "Sub 0, TS 15: Communities: 9, Mod: 0.304, Sizes: [49, 37, 36, 33, 32...]\n",
      "Sub 0, TS 16: Communities: 9, Mod: 0.365, Sizes: [37, 32, 23, 22, 20...]\n",
      "Sub 0, TS 17: Communities: 9, Mod: 0.304, Sizes: [37, 36, 36, 34, 26...]\n",
      "Sub 0, TS 18: Communities: 9, Mod: 0.340, Sizes: [46, 40, 40, 34, 34...]\n",
      "Sub 0, TS 19: Communities: 10, Mod: 0.343, Sizes: [37, 35, 33, 32, 28...]\n",
      "Sub 0, TS 20: Communities: 11, Mod: 0.309, Sizes: [40, 38, 36, 34, 32...]\n",
      "Sub 0, TS 21: Communities: 9, Mod: 0.432, Sizes: [36, 32, 27, 27, 26...]\n",
      "Sub 0, TS 22: Communities: 9, Mod: 0.450, Sizes: [32, 28, 27, 23, 18...]\n",
      "Sub 1, TS 0: Communities: 13, Mod: 0.815, Sizes: [26, 14, 13, 13, 12...]\n",
      "Sub 1, TS 1: Communities: 7, Mod: 0.701, Sizes: [9, 9, 6, 5, 5...]\n",
      "Sub 1, TS 2: Communities: 3, Mod: 0.357, Sizes: [3, 3, 2]\n",
      "Sub 1, TS 3: Communities: 5, Mod: 0.619, Sizes: [7, 6, 5, 5, 3]\n",
      "Sub 1, TS 4: Communities: 3, Mod: 0.426, Sizes: [5, 5, 2]\n",
      "Sub 1, TS 5: Communities: 5, Mod: 0.612, Sizes: [6, 5, 4, 4, 4]\n",
      "Sub 1, TS 6: Communities: 3, Mod: 0.319, Sizes: [3, 2, 2]\n",
      "Sub 1, TS 7: Communities: 2, Mod: 0.219, Sizes: [3, 2]\n",
      "Sub 1, TS 8: Communities: 2, Mod: 0.208, Sizes: [5, 2]\n",
      "Sub 1, TS 9: Communities: 3, Mod: 0.483, Sizes: [4, 4, 4]\n",
      "Sub 2, TS 0: Communities: 7, Mod: 0.635, Sizes: [13, 13, 10, 9, 9...]\n",
      "Sub 2, TS 1: Communities: 8, Mod: 0.654, Sizes: [14, 12, 10, 10, 9...]\n",
      "Sub 2, TS 2: Communities: 10, Mod: 0.737, Sizes: [16, 11, 11, 11, 9...]\n",
      "Sub 2, TS 3: Communities: 9, Mod: 0.759, Sizes: [11, 10, 10, 9, 8...]\n",
      "Sub 2, TS 4: Communities: 8, Mod: 0.710, Sizes: [13, 11, 10, 10, 10...]\n",
      "Sub 2, TS 5: Communities: 7, Mod: 0.692, Sizes: [15, 12, 11, 9, 6...]\n",
      "Sub 2, TS 6: Communities: 7, Mod: 0.760, Sizes: [11, 10, 9, 9, 9...]\n",
      "Sub 2, TS 7: Communities: 11, Mod: 0.821, Sizes: [17, 15, 14, 14, 12...]\n",
      "Sub 2, TS 8: Communities: 7, Mod: 0.708, Sizes: [12, 10, 8, 7, 7...]\n",
      "Sub 2, TS 9: Communities: 4, Mod: 0.622, Sizes: [6, 6, 6, 6]\n",
      "Sub 2, TS 10: Communities: 7, Mod: 0.657, Sizes: [12, 10, 10, 8, 5...]\n",
      "Sub 2, TS 11: Communities: 4, Mod: 0.551, Sizes: [6, 4, 4, 3]\n",
      "Sub 2, TS 12: Communities: 4, Mod: 0.592, Sizes: [7, 6, 5, 4]\n",
      "Sub 2, TS 13: Communities: 5, Mod: 0.585, Sizes: [6, 5, 4, 3, 3]\n",
      "Sub 2, TS 14: Communities: 5, Mod: 0.638, Sizes: [8, 7, 6, 4, 4]\n",
      "Sub 2, TS 15: Communities: 9, Mod: 0.718, Sizes: [13, 13, 12, 12, 10...]\n",
      "Sub 2, TS 16: Communities: 11, Mod: 0.767, Sizes: [21, 18, 16, 14, 14...]\n",
      "Sub 2, TS 17: Communities: 16, Mod: 0.787, Sizes: [36, 26, 25, 22, 21...]\n",
      "Sub 2, TS 18: Communities: 18, Mod: 0.819, Sizes: [38, 28, 26, 23, 23...]\n",
      "Sub 2, TS 19: Communities: 19, Mod: 0.805, Sizes: [33, 27, 26, 25, 23...]\n",
      "Sub 2, TS 20: Communities: 14, Mod: 0.807, Sizes: [31, 25, 24, 23, 21...]\n",
      "Sub 2, TS 21: Communities: 11, Mod: 0.760, Sizes: [28, 22, 15, 13, 12...]\n",
      "Sub 2, TS 22: Communities: 13, Mod: 0.807, Sizes: [36, 21, 20, 17, 16...]\n",
      "Sub 2, TS 23: Communities: 17, Mod: 0.831, Sizes: [33, 24, 20, 19, 17...]\n",
      "Sub 2, TS 24: Communities: 10, Mod: 0.793, Sizes: [20, 19, 16, 14, 12...]\n",
      "Sub 2, TS 25: Communities: 11, Mod: 0.778, Sizes: [25, 15, 14, 11, 11...]\n",
      "Sub 3, TS 0: Communities: 26, Mod: 0.843, Sizes: [58, 53, 43, 37, 36...]\n",
      "Sub 3, TS 1: Communities: 26, Mod: 0.770, Sizes: [61, 57, 47, 47, 45...]\n",
      "Sub 3, TS 2: Communities: 28, Mod: 0.793, Sizes: [54, 54, 51, 47, 44...]\n",
      "Sub 3, TS 3: Communities: 25, Mod: 0.792, Sizes: [58, 56, 49, 43, 40...]\n",
      "Sub 3, TS 4: Communities: 24, Mod: 0.849, Sizes: [58, 43, 40, 38, 38...]\n",
      "Sub 3, TS 5: Communities: 31, Mod: 0.846, Sizes: [57, 43, 39, 35, 35...]\n",
      "Sub 3, TS 6: Communities: 24, Mod: 0.828, Sizes: [42, 36, 32, 31, 31...]\n",
      "Sub 3, TS 7: Communities: 21, Mod: 0.811, Sizes: [35, 32, 31, 29, 28...]\n",
      "Sub 3, TS 8: Communities: 16, Mod: 0.837, Sizes: [30, 20, 20, 17, 15...]\n",
      "Sub 4, TS 0: Communities: 17, Mod: 0.815, Sizes: [32, 20, 19, 18, 17...]\n",
      "Sub 4, TS 1: Communities: 19, Mod: 0.865, Sizes: [39, 30, 28, 28, 27...]\n",
      "Sub 4, TS 2: Communities: 22, Mod: 0.845, Sizes: [56, 45, 42, 40, 37...]\n",
      "Sub 4, TS 3: Communities: 28, Mod: 0.880, Sizes: [72, 43, 41, 41, 39...]\n",
      "Sub 4, TS 4: Communities: 30, Mod: 0.852, Sizes: [84, 66, 47, 40, 39...]\n",
      "Sub 4, TS 5: Communities: 32, Mod: 0.859, Sizes: [78, 56, 55, 52, 50...]\n",
      "Sub 4, TS 6: Communities: 19, Mod: 0.868, Sizes: [31, 27, 25, 25, 24...]\n",
      "Sub 4, TS 7: Communities: 24, Mod: 0.873, Sizes: [45, 44, 43, 42, 41...]\n",
      "Sub 4, TS 8: Communities: 17, Mod: 0.812, Sizes: [35, 28, 27, 27, 24...]\n"
     ]
    }
   ],
   "source": [
    "# Run Leiden community detection on all graphs\n",
    "def analyze_all_communities(graphs_dict):\n",
    "    \"\"\"Run community detection on all graphs and collect statistics\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for sub_id in sorted(graphs_dict.keys()):\n",
    "        ts_dict = graphs_dict[sub_id]\n",
    "        for ts in sorted(ts_dict.keys()):\n",
    "            G = ts_dict[ts]\n",
    "            \n",
    "            if len(G.nodes()) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Convert to igraph\n",
    "            ig_graph, node_to_idx, idx_to_node = nx_to_igraph(G)\n",
    "            \n",
    "            # Run Leiden algorithm\n",
    "            partition = la.find_partition(ig_graph, la.ModularityVertexPartition, seed=42)\n",
    "            \n",
    "            # Calculate statistics\n",
    "            num_communities = len(partition)\n",
    "            modularity = partition.modularity\n",
    "            community_sizes = [len([i for i, comm in enumerate(partition.membership) if comm == c]) \n",
    "                             for c in range(num_communities)]\n",
    "            \n",
    "            results.append({\n",
    "                'subreddit_id': sub_id,\n",
    "                'timestep': ts,\n",
    "                'num_nodes': len(G.nodes()),\n",
    "                'num_edges': len(G.edges()),\n",
    "                'num_communities': num_communities,\n",
    "                'modularity': modularity,\n",
    "                'largest_community': max(community_sizes) if community_sizes else 0,\n",
    "                'smallest_community': min(community_sizes) if community_sizes else 0,\n",
    "                'avg_community_size': sum(community_sizes) / len(community_sizes) if community_sizes else 0,\n",
    "                'community_sizes': community_sizes\n",
    "            })\n",
    "            \n",
    "            print(f\"Sub {sub_id}, TS {ts}: {len(G.nodes())} nodes, {len(G.edges())} edges, \"\n",
    "                  f\"{num_communities} communities, modularity: {modularity:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run analysis on all graphs\n",
    "print(\"=== Community Detection Analysis ===\")\n",
    "community_stats = analyze_all_communities(graph_data.graph_dict)\n",
    "\n",
    "# Summary statistics\n",
    "import pandas as pd\n",
    "stats_df = pd.DataFrame(community_stats)\n",
    "print(\"\\n=== Summary Statistics ===\")\n",
    "print(f\"Total graphs analyzed: {len(stats_df)}\")\n",
    "print(f\"Average modularity: {stats_df['modularity'].mean():.3f}\")\n",
    "print(f\"Average communities per graph: {stats_df['num_communities'].mean():.1f}\")\n",
    "print(f\"Modularity range: {stats_df['modularity'].min():.3f} - {stats_df['modularity'].max():.3f}\")\n",
    "\n",
    "# Show detailed results\n",
    "print(\"\\n=== Detailed Results ===\")\n",
    "for result in community_stats:\n",
    "    sizes_str = ', '.join(map(str, result['community_sizes'][:5]))\n",
    "    if len(result['community_sizes']) > 5:\n",
    "        sizes_str += '...'\n",
    "    print(f\"Sub {result['subreddit_id']}, TS {result['timestep']}: \"\n",
    "          f\"Communities: {result['num_communities']}, \"\n",
    "          f\"Mod: {result['modularity']:.3f}, \"\n",
    "          f\"Sizes: [{sizes_str}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mscenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
